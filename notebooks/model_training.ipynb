{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12474370,"sourceType":"datasetVersion","datasetId":7870299}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm opencv-python-headless scikit-learn \"numpy<2\" --quiet --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T09:58:28.981764Z","iopub.execute_input":"2025-07-17T09:58:28.982288Z","iopub.status.idle":"2025-07-17T10:01:58.187638Z","shell.execute_reply.started":"2025-07-17T09:58:28.982258Z","shell.execute_reply":"2025-07-17T10:01:58.186682Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.7.0 which is incompatible.\nlangchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import functional as TF\nimport timm\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom PIL import Image\nimport os\nimport re\nfrom pathlib import Path\nimport random\nfrom tqdm.auto import tqdm\nimport pickle\nimport json\nfrom collections import defaultdict\nimport gc\nimport sys\n\n# Set seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# Clear GPU memory\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:01:58.189295Z","iopub.execute_input":"2025-07-17T10:01:58.189555Z","iopub.status.idle":"2025-07-17T10:02:06.070098Z","shell.execute_reply.started":"2025-07-17T10:01:58.189532Z","shell.execute_reply":"2025-07-17T10:02:06.069313Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"class Config:\n    DATA_PATH = '/kaggle/input/crop-diseases-2/Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented'\n    MODEL_NAME = 'mobilenetv3_large_100'\n    NUM_CLASSES = 22\n    IMG_SIZE = 224\n    BATCH_SIZE = 64\n    LEARNING_RATE = 2e-4\n    WEIGHT_DECAY = 1e-5\n    EPOCHS = 10\n    N_FOLDS = 5\n    EARLY_STOPPING_PATIENCE = 5\n    SCHEDULER_PATIENCE = 3\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MIXUP_ALPHA = 0.3\n    OUTPUT_DIR = '/kaggle/working'\n    MODELS_DIR = f'{OUTPUT_DIR}/models'\n    PLOTS_DIR = f'{OUTPUT_DIR}/plots'\n    \n    # Memory optimization settings\n    GRADIENT_ACCUMULATION_STEPS = 2\n    USE_MIXED_PRECISION = True\n    MAX_GRAD_NORM = 1.0\n\nos.makedirs(Config.MODELS_DIR, exist_ok=True)\nos.makedirs(Config.PLOTS_DIR, exist_ok=True)\n\n# Enable memory optimization\nif Config.USE_MIXED_PRECISION:\n    from torch.cuda.amp import autocast, GradScaler\n    scaler = GradScaler()\n\nprint(f\"Using device: {Config.DEVICE}\")\nprint(f\"Model: {Config.MODEL_NAME}\")\nprint(f\"Image size: {Config.IMG_SIZE}\")\nprint(f\"Batch size: {Config.BATCH_SIZE}\")\nprint(f\"Mixed precision: {Config.USE_MIXED_PRECISION}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:06.070858Z","iopub.execute_input":"2025-07-17T10:02:06.071367Z","iopub.status.idle":"2025-07-17T10:02:06.133681Z","shell.execute_reply.started":"2025-07-17T10:02:06.071347Z","shell.execute_reply":"2025-07-17T10:02:06.132679Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel: mobilenetv3_large_100\nImage size: 224\nBatch size: 64\nMixed precision: True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def prepare_dataframes(base_path):\n    crops = ['Cashew', 'Cassava', 'Maize', 'Tomato']\n    all_train_files, all_test_files, class_to_idx = [], [], {}\n    current_idx = 0\n    \n    print(\"Scanning directories and cleaning names...\")\n    if not os.path.exists(base_path):\n        print(f\"ERROR: Base path not found at {base_path}.\")\n        return pd.DataFrame(), pd.DataFrame(), {}, {}\n\n    for crop in crops:\n        for phase, file_list in [('train_set', all_train_files), ('test_set', all_test_files)]:\n            crop_path = os.path.join(base_path, crop, phase)\n            if not os.path.exists(crop_path):\n                continue\n            \n            for disease_folder in os.listdir(crop_path):\n                if not os.path.isdir(os.path.join(crop_path, disease_folder)):\n                    continue\n                \n                clean_disease_name = re.sub(r'\\d+$', '', disease_folder).strip()\n                composite_class_name = f\"{crop}_{clean_disease_name}\"\n                \n                if composite_class_name not in class_to_idx:\n                    if phase == 'train_set':\n                        class_to_idx[composite_class_name] = current_idx\n                        current_idx += 1\n                    else:\n                        # Ensure test classes are also in train classes\n                        if composite_class_name in class_to_idx:\n                             pass\n                        else:\n                            continue\n\n                class_idx = class_to_idx.get(composite_class_name)\n                if class_idx is None: continue\n                \n                disease_path = os.path.join(crop_path, disease_folder)\n                \n                for filename in os.listdir(disease_path):\n                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        file_list.append({\n                            'filepath': os.path.join(disease_path, filename),\n                            'class_name': composite_class_name,\n                            'class_idx': class_idx\n                        })\n\n    df_train = pd.DataFrame(all_train_files).sample(frac=1, random_state=42).reset_index(drop=True)\n    df_test = pd.DataFrame(all_test_files).sample(frac=1, random_state=42).reset_index(drop=True)\n    idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n    \n    # Ensure test dataframe only contains classes present in the training set\n    df_test = df_test[df_test['class_idx'].isin(df_train['class_idx'].unique())].reset_index(drop=True)\n\n    Config.NUM_CLASSES = len(class_to_idx)\n    print(f\"\\nFound {Config.NUM_CLASSES} unique classes after cleaning.\")\n    print(f\"Total training images: {len(df_train)}\")\n    print(f\"Total testing images: {len(df_test)}\")\n    \n    return df_train, df_test, class_to_idx, idx_to_class\n\ndf_train, df_test, class_to_idx, idx_to_class = prepare_dataframes(Config.DATA_PATH)\nif class_to_idx:\n    with open(f'{Config.OUTPUT_DIR}/class_mappings.json', 'w') as f:\n        json.dump({'class_to_idx': class_to_idx, 'idx_to_class': idx_to_class}, f)\n    print(\"\\nClass Distribution in Training Data:\")\n    print(df_train['class_name'].value_counts())\nelse:\n    df_train = pd.DataFrame()\n\n\nclass LightweightAugmentation:\n    def __init__(self, img_size=224):\n        self.img_size = img_size\n    \n    def __call__(self, image):\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n        if random.random() < 0.5:\n            image = TF.hflip(image)\n        if random.random() < 0.3:\n            image = TF.vflip(image)\n        if random.random() < 0.4:\n            angle = random.uniform(-15, 15)\n            image = TF.rotate(image, angle)\n        image = TF.resize(image, int(self.img_size * 1.1))\n        i, j, h, w = transforms.RandomCrop.get_params(image, (self.img_size, self.img_size))\n        image = TF.crop(image, i, j, h, w)\n        return image\n\ndef get_transforms(phase='train'):\n    if phase == 'train':\n        return transforms.Compose([\n            LightweightAugmentation(Config.IMG_SIZE),\n            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.2, scale=(0.02, 0.08))\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Lambda(lambda x: Image.fromarray(x) if isinstance(x, np.ndarray) else x),\n            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:06.134741Z","iopub.execute_input":"2025-07-17T10:02:06.135156Z","iopub.status.idle":"2025-07-17T10:02:19.134506Z","shell.execute_reply.started":"2025-07-17T10:02:06.135125Z","shell.execute_reply":"2025-07-17T10:02:19.133372Z"}},"outputs":[{"name":"stdout","text":"Scanning directories and cleaning names...\n\nFound 22 unique classes after cleaning.\nTotal training images: 80271\nTotal testing images: 24981\n\nClass Distribution in Training Data:\nclass_name\nTomato_septoria leaf spot    9373\nCassava_bacterial blight     9195\nCashew_healthy               5877\nTomato_leaf blight           5200\nCashew_red rust              4751\nMaize_streak virus           4043\nMaize_leaf blight            4025\nMaize_leaf beetle            3789\nCashew_leaf miner            3466\nCassava_brown spot           3250\nCassava_green mite           3246\nCashew_anthracnose           3102\nTomato_verticulium wilt      3100\nMaize_leaf spot              3024\nMaize_grasshoper             2575\nCassava_healthy              2271\nCassava_mosaic               2250\nTomato_leaf curl             2050\nTomato_healthy               2000\nCashew_gumosis               1714\nMaize_fall armyworm          1140\nMaize_healthy                 830\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class CropDiseaseDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        label = torch.tensor(row['class_idx'], dtype=torch.long)\n        image = cv2.imread(row['filepath'])\n        if image is None:\n            image = np.zeros((Config.IMG_SIZE, Config.IMG_SIZE, 3), dtype=np.uint8)\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:19.137862Z","iopub.execute_input":"2025-07-17T10:02:19.138215Z","iopub.status.idle":"2025-07-17T10:02:19.146087Z","shell.execute_reply.started":"2025-07-17T10:02:19.138181Z","shell.execute_reply":"2025-07-17T10:02:19.145130Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MobileNetV3CropModel(nn.Module):\n    def __init__(self, model_name=Config.MODEL_NAME, num_classes=Config.NUM_CLASSES, pretrained=True):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        num_features = self.backbone.num_features\n        self.backbone.reset_classifier(0)\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.BatchNorm1d(num_features),\n            nn.Dropout(0.2),\n            nn.Linear(num_features, num_features // 4),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(num_features // 4),\n            nn.Dropout(0.1),\n            nn.Linear(num_features // 4, num_classes)\n        )\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.classifier.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        features = self.backbone.forward_features(x)\n        out = self.classifier(features)\n        return out\n        \nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience, self.verbose, self.delta = patience, verbose, delta\n        self.counter, self.best_score, self.early_stop = 0, None, False\n        self.val_loss_min = np.Inf\n        self.best_weights = None\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose: print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience: self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n            self.save_checkpoint(val_loss, model)\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose: print(f'Val loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n        self.best_weights = {k: v.cpu() for k, v in model.state_dict().items()}\n        self.val_loss_min = val_loss\n\n    def load_best_weights(self, model):\n        if self.best_weights:\n            model.load_state_dict({k: v.to(Config.DEVICE) for k, v in self.best_weights.items()})\n        return model\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, num_classes, smoothing=0.1):\n        super().__init__()\n        self.num_classes, self.smoothing = num_classes, smoothing\n        self.confidence = 1.0 - smoothing\n    \n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n\ndef mixup_data(x, y, alpha=0.3):\n    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:19.147421Z","iopub.execute_input":"2025-07-17T10:02:19.147739Z","iopub.status.idle":"2025-07-17T10:02:19.167781Z","shell.execute_reply.started":"2025-07-17T10:02:19.147712Z","shell.execute_reply":"2025-07-17T10:02:19.167057Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n    model.train()\n    running_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    \n    optimizer.zero_grad()\n    \n    for batch_idx, (data, target) in enumerate(tqdm(loader, desc='[Train]')):\n        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n        \n        # Use mixed precision if available\n        if Config.USE_MIXED_PRECISION and scaler is not None:\n            with autocast():\n                # Apply mixup occasionally\n                if np.random.rand() < 0.3 and Config.MIXUP_ALPHA > 0:\n                    mixed_data, y_a, y_b, lam = mixup_data(data, target, Config.MIXUP_ALPHA)\n                    output = model(mixed_data)\n                    loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n                else:\n                    output = model(data)\n                    loss = criterion(output, target)\n                \n                # Normalize loss by gradient accumulation steps\n                loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n            \n            scaler.scale(loss).backward()\n            \n            # Gradient accumulation\n            if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        \n        else:\n            # Regular training without mixed precision\n            if np.random.rand() < 0.3 and Config.MIXUP_ALPHA > 0:\n                mixed_data, y_a, y_b, lam = mixup_data(data, target, Config.MIXUP_ALPHA)\n                output = model(mixed_data)\n                loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n            else:\n                output = model(data)\n                loss = criterion(output, target)\n            \n            loss = loss / Config.GRADIENT_ACCUMULATION_STEPS\n            loss.backward()\n            \n            if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n                optimizer.step()\n                optimizer.zero_grad()\n        \n        running_loss += loss.item() * Config.GRADIENT_ACCUMULATION_STEPS * data.size(0)\n        total_samples += target.size(0)\n        \n        # Calculate accuracy (only for non-mixup batches)\n        if np.random.rand() >= 0.3 or Config.MIXUP_ALPHA <= 0:\n            _, predicted = torch.max(output.data, 1)\n            total_correct += (predicted == target).sum().item()\n        \n        # Clear cache periodically\n        if batch_idx % 20 == 0:\n            torch.cuda.empty_cache()\n    \n    epoch_loss = running_loss / total_samples\n    epoch_acc = (total_correct / total_samples) * 100\n    \n    return epoch_loss, epoch_acc\n\ndef validate_one_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    \n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(tqdm(loader, desc='[Val]')):\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            \n            if Config.USE_MIXED_PRECISION:\n                with autocast():\n                    output = model(data)\n                    loss = criterion(output, target)\n            else:\n                output = model(data)\n                loss = criterion(output, target)\n            \n            running_loss += loss.item() * data.size(0)\n            total_samples += target.size(0)\n            \n            _, predicted = torch.max(output.data, 1)\n            total_correct += (predicted == target).sum().item()\n            \n            # Clear cache periodically\n            if batch_idx % 20 == 0:\n                torch.cuda.empty_cache()\n    \n    epoch_loss = running_loss / total_samples\n    epoch_acc = (total_correct / total_samples) * 100\n    \n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:19.168823Z","iopub.execute_input":"2025-07-17T10:02:19.169909Z","iopub.status.idle":"2025-07-17T10:02:19.191179Z","shell.execute_reply.started":"2025-07-17T10:02:19.169878Z","shell.execute_reply":"2025-07-17T10:02:19.190424Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_kfold(df_train, n_folds=Config.N_FOLDS):\n    if df_train.empty:\n        print(\"Training df empty. Halting.\")\n        return [], None\n    \n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    results = []\n    oof_preds = np.zeros((len(df_train), Config.NUM_CLASSES))\n    \n    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train['class_idx'])):\n        print(f\"\\n{'='*20} FOLD {fold+1}/{n_folds} {'='*20}\")\n        \n        # Clear memory before each fold\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        train_df = df_train.iloc[train_idx]\n        val_df = df_train.iloc[val_idx]\n        \n        # Data loaders with memory optimization\n        train_loader = DataLoader(\n            CropDiseaseDataset(train_df, get_transforms('train')),\n            batch_size=Config.BATCH_SIZE,\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True,\n            persistent_workers=True\n        )\n        \n        val_loader = DataLoader(\n            CropDiseaseDataset(val_df, get_transforms('val')),\n            batch_size=Config.BATCH_SIZE * 2,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True,\n            persistent_workers=True\n        )\n        \n        # Initialize model\n        model = MobileNetV3CropModel().to(Config.DEVICE)\n        \n        # Optimizer with different learning rates for backbone and classifier\n        backbone_params = [p for name, p in model.named_parameters() if 'backbone' in name]\n        classifier_params = [p for name, p in model.named_parameters() if 'classifier' in name]\n        \n        optimizer = torch.optim.AdamW([\n            {'params': backbone_params, 'lr': Config.LEARNING_RATE * 0.1},  # Lower LR for pretrained backbone\n            {'params': classifier_params, 'lr': Config.LEARNING_RATE}\n        ], weight_decay=Config.WEIGHT_DECAY)\n        \n        # Learning rate scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, T_0=5, T_mult=2, eta_min=1e-6\n        )\n        \n        # Loss function with label smoothing\n        criterion = LabelSmoothingLoss(Config.NUM_CLASSES, smoothing=0.1).to(Config.DEVICE)\n        early_stopping = EarlyStopping(patience=Config.EARLY_STOPPING_PATIENCE, verbose=True)\n        \n        # Mixed precision scaler\n        fold_scaler = GradScaler() if Config.USE_MIXED_PRECISION else None\n        \n        history = defaultdict(list)\n        \n        for epoch in range(Config.EPOCHS):\n            print(f\"Epoch {epoch+1}/{Config.EPOCHS}\")\n            \n            train_loss, train_acc = train_one_epoch(\n                model, train_loader, criterion, optimizer, Config.DEVICE, fold_scaler\n            )\n            val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, Config.DEVICE)\n            \n            print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n            \n            history['train_loss'].append(train_loss)\n            history['val_loss'].append(val_loss)\n            history['train_acc'].append(train_acc)\n            history['val_acc'].append(val_acc)\n            \n            scheduler.step()\n            early_stopping(val_loss, model)\n            \n            if early_stopping.early_stop:\n                print(\"Early stopping triggered.\")\n                break\n        \n        # Load best weights and save model\n        model = early_stopping.load_best_weights(model)\n        torch.save(model.state_dict(), f'{Config.MODELS_DIR}/mobilenet_model_fold_{fold+1}.pth')\n        \n        # Generate OOF predictions with memory optimization\n        model.eval()\n        fold_preds = []\n        with torch.no_grad():\n            for batch_idx, (data, _) in enumerate(tqdm(val_loader, desc=\"OOF Preds\")):\n                data = data.to(Config.DEVICE, non_blocking=True)\n                \n                if Config.USE_MIXED_PRECISION:\n                    with autocast():\n                        output = model(data)\n                else:\n                    output = model(data)\n                \n                probabilities = F.softmax(output, dim=1)\n                fold_preds.append(probabilities.cpu().numpy())\n                \n                # Clear cache\n                if batch_idx % 10 == 0:\n                    torch.cuda.empty_cache()\n        \n        oof_preds[val_idx] = np.concatenate(fold_preds)\n        \n        results.append({'history': history})\n        print(f\"Fold {fold+1} Best Val Acc: {np.max(history['val_acc']):.2f}%\")\n        \n        # Clean up memory\n        del model, train_loader, val_loader, optimizer, scheduler, criterion\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    cv_mean_acc = np.mean([np.max(fold_result['history']['val_acc']) for fold_result in results])\n    print(f\"\\nCV Mean Best Acc: {cv_mean_acc:.2f}%\")\n    \n    # Save OOF predictions\n    np.save(f'{Config.OUTPUT_DIR}/oof_predictions_mobilenet.npy', oof_preds)\n    \n    return results, oof_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:19.192022Z","iopub.execute_input":"2025-07-17T10:02:19.192582Z","iopub.status.idle":"2025-07-17T10:02:19.209803Z","shell.execute_reply.started":"2025-07-17T10:02:19.192556Z","shell.execute_reply":"2025-07-17T10:02:19.209039Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"fold_results, oof_predictions = train_kfold(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:02:19.210560Z","iopub.execute_input":"2025-07-17T10:02:19.210795Z","execution_failed":"2025-07-17T10:12:41.288Z"}},"outputs":[{"name":"stdout","text":"\n==================== FOLD 1/5 ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2d27113d0440539f426861d7c57d85"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Train]:   0%|          | 0/1004 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68c2e6c9f0340c18a345700a1e6fe9e"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(fold_results):\n    if not fold_results:\n        return\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    for i, result in enumerate(fold_results):\n        history = result['history']\n        axes[0].plot(history['val_loss'], label=f'Fold {i+1} Val Loss')\n        axes[1].plot(history['val_acc'], label=f'Fold {i+1} Val Acc')\n    \n    axes[0].set_title('Validation Loss - MobileNetV3')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    axes[1].set_title('Validation Accuracy - MobileNetV3')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy (%)')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(f'{Config.PLOTS_DIR}/mobilenet_training_history.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef analyze_oof_predictions(oof_preds, df, idx_to_class_map):\n    if oof_preds is None or df.empty or not idx_to_class_map:\n        return\n    \n    true_labels = df['class_idx'].values\n    pred_labels = np.argmax(oof_preds, axis=1)\n    \n    target_names = [idx_to_class_map[i] for i in sorted(idx_to_class_map.keys())]\n    \n    oof_accuracy = accuracy_score(true_labels, pred_labels) * 100\n    print(f\"\\nOOF Accuracy (MobileNetV3): {oof_accuracy:.2f}%\")\n    \n    # Confusion Matrix\n    cm = confusion_matrix(true_labels, pred_labels)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=target_names, yticklabels=target_names, cbar=False)\n    plt.title('MobileNetV3 Out-of-Fold Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(f'{Config.PLOTS_DIR}/mobilenet_oof_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Plot results\nplot_training_history(fold_results)\nanalyze_oof_predictions(oof_predictions, df_train, idx_to_class)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MobileNetEnsemble:\n    def __init__(self, model_paths, device):\n        self.models = []\n        self.device = device\n        \n        for path in model_paths:\n            if os.path.exists(path):\n                model = self.load_model(path, device)\n                self.models.append(model)\n        \n        print(f\"Loaded {len(self.models)} MobileNetV3 models for ensemble.\")\n    \n    def load_model(self, path, device):\n        model = MobileNetV3CropModel(pretrained=False)\n        model.load_state_dict(torch.load(path, map_location=device))\n        model.to(device)\n        model.eval()\n        return model\n    \n    def predict(self, loader):\n        if not self.models:\n            return None\n        \n        all_predictions = []\n        \n        with torch.no_grad():\n            for batch_idx, (data, _) in enumerate(tqdm(loader, desc=\"Ensemble Inference\")):\n                data = data.to(self.device, non_blocking=True)\n                \n                # Get predictions from all models\n                batch_predictions = []\n                for model in self.models:\n                    if Config.USE_MIXED_PRECISION:\n                        with autocast():\n                            output = model(data)\n                    else:\n                        output = model(data)\n                    \n                    probabilities = F.softmax(output, dim=1)\n                    batch_predictions.append(probabilities)\n                \n                # Average predictions\n                ensemble_pred = torch.stack(batch_predictions).mean(dim=0)\n                all_predictions.append(ensemble_pred.cpu().numpy())\n                \n                # Clear cache\n                if batch_idx % 10 == 0:\n                    torch.cuda.empty_cache()\n        \n        return np.concatenate(all_predictions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_paths = [f'{Config.MODELS_DIR}/mobilenet_model_fold_{i+1}.pth' for i in range(Config.N_FOLDS)]\nensemble_model = MobileNetEnsemble(model_paths, Config.DEVICE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_test_set(df_test, ensemble, idx_to_class_map):\n    if df_test.empty or not hasattr(ensemble, 'models') or not ensemble.models or not idx_to_class_map:\n        print(\"Skipping test evaluation - missing data or models.\")\n        return\n    \n    print(f\"\\n{'='*20} Evaluating MobileNetV3 on Test Set {'='*20}\")\n    \n    test_loader = DataLoader(\n        CropDiseaseDataset(df_test, get_transforms('val')),\n        batch_size=Config.BATCH_SIZE * 2, # Use larger batch for inference\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    test_predictions = ensemble.predict(test_loader)\n    \n    if test_predictions is not None:\n        true_labels = df_test['class_idx'].values\n        pred_labels = np.argmax(test_predictions, axis=1)\n        \n        target_names = [idx_to_class_map[i] for i in sorted(idx_to_class_map.keys())]\n        \n        test_accuracy = accuracy_score(true_labels, pred_labels) * 100\n        print(f\"\\nFinal Test Set Accuracy (Ensemble): {test_accuracy:.2f}%\")\n        \n        print(\"\\nClassification Report (Test Set):\")\n        print(classification_report(true_labels, pred_labels, target_names=target_names, digits=3))\n        \n        # Confusion Matrix for Test Set\n        cm = confusion_matrix(true_labels, pred_labels)\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n                    xticklabels=target_names, yticklabels=target_names, cbar=False)\n        plt.title('MobileNetV3 Test Set Confusion Matrix (Ensemble)')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n        plt.tight_layout()\n        plt.savefig(f'{Config.PLOTS_DIR}/mobilenet_test_confusion_matrix.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\nevaluate_test_set(df_test, ensemble_model, idx_to_class)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n# This cell creates a self-contained prediction pipeline class and saves it using joblib.\n# This is the recommended way to prepare a model for deployment in a web app.\n\nclass PredictionPipeline:\n    def __init__(self, model, transforms, idx_to_class):\n        # For deployment, it's best practice to force the model to CPU\n        self.device = torch.device('cpu')\n        self.model = model\n        # Ensure all models in the ensemble are on the CPU and in eval mode\n        for m in self.model.models:\n            m.to(self.device)\n            m.eval()\n        self.transforms = transforms\n        self.idx_to_class = idx_to_class\n\n    def predict(self, image_path):\n        \"\"\"\n        Takes the path to an image, preprocesses it, and returns the predicted class and confidence.\n        \"\"\"\n        # 1. Load and preprocess image\n        image = cv2.imread(image_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found at {image_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 2. Apply transforms\n        transformed_image = self.transforms(image)\n        # Add batch dimension and move to CPU\n        tensor = transformed_image.unsqueeze(0).to(self.device)\n\n        # 3. Make prediction\n        with torch.no_grad():\n            # Get raw logits from each model in the ensemble\n            batch_predictions = [m(tensor) for m in self.model.models]\n            # Average the logits and apply softmax for probabilities\n            ensemble_pred = torch.stack(batch_predictions).mean(dim=0)\n            probabilities = F.softmax(ensemble_pred, dim=1)\n        \n        # 4. Get top prediction and confidence\n        confidence, pred_idx = torch.max(probabilities, 1)\n        pred_idx = pred_idx.item()\n        \n        # 5. Map index to class name\n        predicted_class = self.idx_to_class.get(pred_idx, \"Unknown Class\")\n        \n        return predicted_class, confidence.item() * 100\n\n# --- Create and save the pipeline ---\n\n# 1. Get the necessary components\ninference_transforms = get_transforms('val')\n# Ensure integer keys for the map\nint_idx_to_class = {int(k): v for k, v in idx_to_class.items()}\n\n# 2. IMPORTANT: Create a new ensemble instance on the CPU for deployment\n# This prevents saving the model with GPU dependencies.\nensemble_model_cpu = MobileNetEnsemble(model_paths, device=torch.device('cpu'))\n\n# 3. Instantiate the pipeline\ndeployment_pipeline = PredictionPipeline(\n    model=ensemble_model_cpu,\n    transforms=inference_transforms,\n    idx_to_class=int_idx_to_class\n)\n\n# 4. Save the entire pipeline object with joblib\npipeline_path = f'{Config.OUTPUT_DIR}/mobilenet_deployment_pipeline.joblib'\njoblib.dump(deployment_pipeline, pipeline_path)\n\nprint(f\"\\n✅ Deployment pipeline saved successfully to: {pipeline_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This cell shows how to load the .joblib file and use it for prediction.\n# You would use this same logic in your web application's backend.\n\nprint(\"--- Testing the saved deployment pipeline ---\")\n\n# 1. Load the saved pipeline\ntry:\n    loaded_pipeline = joblib.load(pipeline_path)\n    print(\"Pipeline loaded successfully.\")\n\n    # 2. Get a random image from the test set to test the pipeline\n    if not df_test.empty:\n        sample_row = df_test.sample(1).iloc[0]\n        sample_image_path = sample_row['filepath']\n        true_label = sample_row['class_name']\n        \n        print(f\"\\nTesting with image: {sample_image_path}\")\n        print(f\"True Label: {true_label}\")\n\n        # 3. Make a prediction\n        predicted_class, confidence = loaded_pipeline.predict(sample_image_path)\n        \n        # Display image\n        img = Image.open(sample_image_path)\n        plt.imshow(img)\n        plt.title(f\"Predicted: {predicted_class} ({confidence:.2f}%)\\nTrue: {true_label}\")\n        plt.axis('off')\n        plt.show()\n\n        print(f\"\\nPredicted Disease: {predicted_class}\")\n        print(f\"Confidence: {confidence:.2f}%\")\n    else:\n        print(\"Test dataframe is empty, cannot demonstrate prediction.\")\n\nexcept FileNotFoundError:\n    print(f\"Error: Could not find the pipeline file at {pipeline_path}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-17T10:12:41.294Z"}},"outputs":[],"execution_count":null}]}